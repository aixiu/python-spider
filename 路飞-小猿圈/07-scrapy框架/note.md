# scrapy框架

## 什么是框架

- 就是集成了很多功能，并且具有很强通用性的一个项目模板。

## 如何学习框架

- 专门学习框架封装的各种功能的详细用法。

## 什么是scrapy

- 爬虫中封装好的一个明星框架。
- 功能：高性能的持久化存储，异步数据下载，高性能的数据解析，分布式等常用功能。

## scrapy框架的基本使用

- 环境安装：pip install scrapy
- 安装：pip install pywin32
- pywin32是Python的一个代码库，包装了Windows 系统的 Win32 API，能创建和使用 COM 对象和图形窗口界面。如果你想用Python操控Windows系统，创建窗口、接受键鼠命令，或用到Win32 API，那你就需要它。
- 创建一个工程  scrapy startproject XXXPro
- 一定要先进入XXXPro工程目录
- 在spiders子目录中创一个爬虫文件
  - scrapy genspider spiderName www.xxx.com
- 执行工程
  - scrapy crawl spiderName
  - 工程配置文件：# 显示指定类型的日志信息  LOG_LEVEL = 'ERROR'

## scrapy数据解析

## scrapy持久化存储

- 基于终端指令：
  - 要求：只可以将parse方法的返回值存储到本地的文本文件中
  - 持久化存储对应的文本文件的类型只可以为：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'
  - 指令：scrapy crawl xxx -o filePath
  - 好处：简洁高效便捷
  - 缺点：局限性比较强
    - 数据只可以存储到指定后缀的文本文件中
- 基于管道：
  - 编码流程：
    - 数据解折
    - 在item类中定义相关的属性
    - 将解析的数据封装存储到item类型的对象
    - 将item类型的对象提交给管道进行持久华存储的操作
    - 在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化操作
    - 在配置文件中开启管道
  - 好处：
    - 通用性强。

- 面试题：将爬取到的数据一份存储到本地一份存储到数据库，如何实现？
  - 管道文件中一个管道类对应的是将数据存储到一种平台
  - 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接收
  - process_itme中的 retun item 表示将item传递给下一个即将被执行的管道类

- 基于Spider的全站数据爬取
  - 就是将网站中某版块下的全部页码对应的页面数据进行爬取
  - 需求：爬取校花网中的照片名称
  - 实现方式：
    - 将所有页面的URL添加到start_urls列表（不推荐）
    - 自行手动进行请求发送（）
      - 手动请求发送：
        - scrapy.Request(url, callback)  callback:专门用做于数据解析

- 一个例子：
  - title：如何利用Python爬虫爬取网页中图片(成功实现自动翻页至最后一页)
  - url：<https://blog.csdn.net/weixin_65423581/article/details/122533646>
  - 判断next_url是否存在,若不存在返回None值以便结束循环(需要学习的点)

- 五大核心组件
  - （1）引擎(Scrapy)
    - 用来处理整个系统的数据流处理, 触发事务(框架核心)
  - （2）调度器(Scheduler)
    - 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
  - （3）下载器(Downloader)（scrapy的异步在这里）
    - 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
  -（4）爬虫(Spiders)
    - 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
  - （5）项目管道(Pipeline)
    - 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。

- scrapy五大核心组件之间的工作流程：
  - （1）首先，我们最原始的起始url是在我们爬虫文件中的，通常情况系，起始的url只有一个，当我们的爬虫文件执行的时候，首先对起始url发送请求，将起始url封装成了请求对象，将请求对象传递给了引擎，引擎就收到了爬虫文件给它发送的封装了起始URL的请求对象。我们在爬虫文件中发送的请求并没有拿到响应（没有马上拿到响应），只有请求发送到服务器端，服务器端返回响应，才能拿到响应。
  - （2）引擎拿到这个请求对象以后，又将请求对象发送给了调度器，队列接受到的请求都放到了队列当中，队列中可能存在多个请求对象，然后通过过滤器，去掉重复的请求
  - （3）调度器将过滤后的请求对象发送给了引擎，
  - （4）引擎将拿到的请求对象给了下载器
  - （5）下载器拿到请求后将请求拿到互联网进行数据下载
  - （6）互联网将下载好的数据发送给下载器，此时下载好的数据是封装在响应对象中的
  - （7）下载器将响应对象发送给引擎，引擎接收到了响应对象，此时引擎中存储了从互联网中下载的数据。
  - （8）最终，这个响应对象又由引擎给了spider（爬虫文件），由parse方法中的response对象来接收，然后再parse方法中进行解析数据，此时可能解析到新的url，然后再次发请求；也可能解析到相关的数据，然后将数据进行封装得到item，
  - （9）spider将item发送给引擎，
  - （10）引擎将item发送给管道。

- 其中，在引擎和下载中间还有一个下载器中间件，spider和引擎中间有爬虫中间件:
  - 下载器中间件
    - 可以拦截请求和响应对象，请求和响应交互的时候一定会经过下载中间件，可以处理请求和响应。
  - 爬虫中间件
    - 拦截请求和响应，对请求和响应进行处理。

- 原文链接：<https://blog.csdn.net/weixin_45890771/article/details/122841485>

- 请求传参
  - 使用场景：如果要爬取解析的数据不在同一张页面中。（深度爬取）
  - 需求：爬取boss的岗位名称，岗位描述。
